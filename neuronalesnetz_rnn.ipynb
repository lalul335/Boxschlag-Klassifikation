{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-26T09:22:05.983171Z",
     "start_time": "2024-06-26T09:22:05.918646Z"
    }
   },
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import data_preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import scipy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from tsfresh import extract_features\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tsfresh'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecomposition\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PCA\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtsfresh\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m extract_features\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tsfresh'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T09:21:46.435878Z",
     "start_time": "2024-06-26T09:21:40.245531Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install scikit-learn",
   "id": "85a061e31c82fab6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\fiete\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\fiete\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\fiete\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\fiete\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\fiete\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T09:22:10.534505Z",
     "start_time": "2024-06-26T09:22:10.234058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def csv_to_json(csv_file_path, json_file_path):\n",
    "    # Read the CSV data\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    print(\"moin\")\n",
    "    # Convert the CSV data to JSON  \n",
    "    json_data = df.to_json(orient='records')\n",
    "\n",
    "    # Write the JSON data to a file\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json_file.write(json_data)\n",
    "\n",
    "\n",
    "path_dataset = r'small_dataset(3).csv'\n",
    "path_json = r'max_dataset.json'\n",
    "csv_to_json(path_dataset, path_json)"
   ],
   "id": "694baa5c53790bca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moin\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T09:22:13.839596Z",
     "start_time": "2024-06-26T09:22:13.680897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalisierung der Daten\n",
    "\"\"\"\n",
    "Gerne mit periodLengthMS und sampling sampleRateUS rumspielen. Den Effekt von den Variablen könnt ihr in smartPunch_tutorial_ManipulatePeriodLength.ipynb sehen.  \n",
    "\"\"\"\n",
    "import json\n",
    "\n",
    "periodLengthMS = 1000\n",
    "sampleRateUS = 1000\n",
    "\n",
    "\n",
    "with open('max_dataset.json', 'r') as f:\n",
    "    dsds = json.load(f)\n",
    "\n",
    "print(\"moin\")\n",
    "\n",
    "\n",
    "ds = data_preprocessing.jsonData_to_dataset_in_timewdifference_us(dsds)\n",
    "\n",
    "\n",
    "\n",
    "df_new = data_preprocessing.normate_dataset_period(periodLengthMS, sampleRateUS, ds)"
   ],
   "id": "570969bc05df4879",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moin\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'data_preprocessing' has no attribute 'jsonData_to_dataset_in_timedifference_us'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 17\u001B[0m\n\u001B[0;32m     12\u001B[0m     dsds \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmoin\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 17\u001B[0m ds \u001B[38;5;241m=\u001B[39m \u001B[43mdata_preprocessing\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjsonData_to_dataset_in_timedifference_us\u001B[49m(dsds)\n\u001B[0;32m     21\u001B[0m df_new \u001B[38;5;241m=\u001B[39m data_preprocessing\u001B[38;5;241m.\u001B[39mnormate_dataset_period(periodLengthMS, sampleRateUS, ds)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'data_preprocessing' has no attribute 'jsonData_to_dataset_in_timedifference_us'"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = pd.DataFrame({'idx': range(len(df_new)), 'dfs': df_new})",
   "id": "373c95381e08d49e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gerade = []\n",
    "kinnhaken = []\n",
    "kopfhaken = []\n",
    "\n",
    "#size_of_trainset = int(len(df) * 0.2)\n",
    "#count_per_label = int(size_of_trainset / 3)\n",
    "\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if(df.iloc[i][1].iloc[1].label == \"Gerade\"):\n",
    "        kopfhaken.append(df.iloc[i][1])\n",
    "    elif(df.iloc[i][1].iloc[1].label == \"Kinnhaken\"):\n",
    "        kinnhaken.append(df.iloc[i][1])\n",
    "    elif(df.iloc[i][1].iloc[1].label == \"Kopfhaken\"):\n",
    "        gerade.append(df.iloc[i][1])\n",
    "\n",
    "\n",
    "traindata = gerade+ kinnhaken+ kopfhaken\n",
    "\n",
    "print(len(traindata))"
   ],
   "id": "db83823f47b50d53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x_train,y_train = [],[]\n",
    "\n",
    "for df in traindata:\n",
    "    y_train.append(df[\"label\"].iloc[0])\n",
    "    x_train.append(df.drop(columns=[\"timestamp\", \"label\"]).values)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_train.shape, y_train.shape"
   ],
   "id": "51a7487bab6f5517"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "LABELS = ['Gerade', 'Kinnhaken', 'Kopfhaken']\n",
    "# Set classes to visualize and set the feature names\n",
    "FEATURE_NAMES = ['x', 'y', 'z']\n",
    "CLASSES = ['Gerade', 'Kinnhaken', 'Kopfhaken']\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=len(CLASSES), sharex=True, sharey=False)\n",
    "fig.suptitle('Visualization of different Classes', fontsize=16)\n",
    "fig.set_size_inches(16, 9)\n",
    "x = np.arange(1001)\n",
    "for index, cls in enumerate(CLASSES):\n",
    "    # Set class as label for top subplot of each column\n",
    "    axs[0][index].set(title=cls)\n",
    "    # Get random instance from training data with corresponding class\n",
    "    instance_index = np.random.choice(np.argwhere(y_train == cls).flatten())\n",
    "    instance = x_train[instance_index]\n",
    "    # Plot the feature groups (all axis of one measurement) in the different subplots and add labels\n",
    "    for sublot in [0, 1, 2]:\n",
    "        axs[sublot][index].plot(x, instance[:, sublot])\n",
    "        axs[sublot][index].legend(FEATURE_NAMES[sublot])\n",
    "plt.show()"
   ],
   "id": "e277c228495f3fe3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "_label = ['Gerade', 'Kinnhaken', 'Kopfhaken']\n",
    "category_to_num = {element: num for num, element in enumerate(_label)}\n",
    "numerical_data = np.vectorize(category_to_num.get)(y_train)\n",
    "\n",
    "\n",
    "y_train  = np.array(numerical_data)"
   ],
   "id": "5944d9ce943024c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Definiere das Modell\n",
    "model = Sequential([\n",
    "    InputLayer(shape=x_train[1].shape),\n",
    "    SimpleRNN(units=256, activation='relu', return_sequences=True),\n",
    "    Dropout(rate=0.2),  # Dropout zur Regularisierung\n",
    "    SimpleRNN(units=256, activation='relu', return_sequences=False),\n",
    "    Dropout(rate=0.2),  # Dropout zur Regularisierung\n",
    "    Dense(3, activation='softmax')  # Ausgangsschicht für 3 Klassen\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),  # Angepasste Lernrate\n",
    "              loss=CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Zusammenfassung des Modells ausdrucken\n",
    "print(model.summary())"
   ],
   "id": "dec0fe1c32fbd796"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set parameters for data splitting and training\n",
    "TEST_SIZE = 0.2\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 30\n",
    "\n",
    "\n",
    "# Encode the labels using One-Hot-Encoding\n",
    "y_train_encoded = tf.one_hot(indices=y_train, depth=3)\n",
    "\n",
    "\n",
    "# Train model using validation split\n",
    "history = model.fit(x=x_train, y=y_train_encoded, validation_split=TEST_SIZE, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "# Plot the training histroy\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.set_size_inches(12, 8)\n",
    "fig.suptitle('Training History', fontsize=16)\n",
    "axs[0].plot(history.epoch, history.history['loss'], history.history['val_loss'])\n",
    "axs[0].set(title='Loss', xlabel='Epoch', ylabel='Loss')\n",
    "axs[0].legend(['loss', 'val_loss'])\n",
    "axs[1].plot(history.epoch, history.history['accuracy'], history.history['val_accuracy'])\n",
    "axs[1].set(title='Accuracy', xlabel='Epoch', ylabel='Accuracy')\n",
    "axs[1].legend(['accuracy', 'val_accuracy'])\n",
    "plt.show()"
   ],
   "id": "e2a2e1b88e8d5720"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_predicted = np.argmax(model.predict(x=x_train), axis=1)\n",
    "confusion_matrix = tf.math.confusion_matrix(labels=y_train, predictions=y_predicted)\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(12, 8)\n",
    "sns.heatmap(confusion_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ],
   "id": "f427e9b7ad2f28c8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
