{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-26T14:06:46.563297Z",
     "start_time": "2024-06-26T14:06:38.124730Z"
    }
   },
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import data_preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import scipy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "#from tsfresh import extract_features\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T09:21:46.435878Z",
     "start_time": "2024-06-26T09:21:40.245531Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install scikit-learn",
   "id": "85a061e31c82fab6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\fiete\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\fiete\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\fiete\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\fiete\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\fiete\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T14:07:00.233555Z",
     "start_time": "2024-06-26T14:07:00.220584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def csv_to_json(csv_file_path, json_file_path):\n",
    "    # Read the CSV data\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Convert the CSV data to JSON\n",
    "    json_data = df.to_json(orient='records')\n",
    "\n",
    "    # Write the JSON data to a file\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json_file.write(json_data)\n",
    "    \n",
    "\n"
   ],
   "id": "694baa5c53790bca",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T14:07:02.827119Z",
     "start_time": "2024-06-26T14:07:02.597330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path_dataset = r'small_dataset.csv'\n",
    "path_json = r'C:\\Users\\fiete\\PycharmProjects\\boxklassifikation\\Boxschlag-Klassifikation\\set1.json'\n",
    "csv_to_json(path_dataset, path_json)"
   ],
   "id": "4f641080ab130f24",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T14:13:47.548047Z",
     "start_time": "2024-06-26T14:13:47.427072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalisierung der Daten\n",
    "\"\"\"\n",
    "Gerne mit periodLengthMS und sampling sampleRateUS rumspielen. Den Effekt von den Variablen könnt ihr in smartPunch_tutorial_ManipulatePeriodLength.ipynb sehen.  \n",
    "\"\"\"\n",
    "import json\n",
    "import prepro\n",
    "\n",
    "periodLengthMS = 1000\n",
    "sampleRateUS = 1000\n",
    "\n",
    "\n",
    "with open('set1.json', 'r') as f:\n",
    "    dsds = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ds = prepro.jsonData_to_dataset_in_timedifference_us(dsds)\n",
    "\n",
    "print(\"moin\")\n",
    "\n",
    "df_new = prepro.normate_dataset_period(periodLengthMS, sampleRateUS, ds)"
   ],
   "id": "570969bc05df4879",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 19\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mset1.json\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m     13\u001B[0m     dsds \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[1;32m---> 19\u001B[0m ds \u001B[38;5;241m=\u001B[39m \u001B[43mprepro\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjsonData_to_dataset_in_timedifference_us\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdsds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmoin\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     23\u001B[0m df_new \u001B[38;5;241m=\u001B[39m prepro\u001B[38;5;241m.\u001B[39mnormate_dataset_period(periodLengthMS, sampleRateUS, ds)\n",
      "File \u001B[1;32m~\\PycharmProjects\\boxklassifikation\\Boxschlag-Klassifikation\\prepro.py:407\u001B[0m, in \u001B[0;36mjsonData_to_dataset_in_timedifference_us\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m    404\u001B[0m \u001B[38;5;66;03m# raw_time_us = 0\u001B[39;00m\n\u001B[0;32m    405\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m raw \u001B[38;5;129;01min\u001B[39;00m value[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mraws\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[0;32m    406\u001B[0m     \u001B[38;5;66;03m# raw_time_us += int(raw['timestamp'])//1000\u001B[39;00m\n\u001B[1;32m--> 407\u001B[0m     t \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[43mraw\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtimestamp\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m1000\u001B[39m\n\u001B[0;32m    408\u001B[0m     the_raws\u001B[38;5;241m.\u001B[39mappend([raw[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m'\u001B[39m], raw[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m'\u001B[39m], raw[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mz\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[38;5;28mint\u001B[39m(\n\u001B[0;32m    409\u001B[0m         t), value[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]])\n\u001B[0;32m    410\u001B[0m     the_indxs\u001B[38;5;241m.\u001B[39mappend(idx)\n",
      "\u001B[1;31mTypeError\u001B[0m: string indices must be integers"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = pd.DataFrame({'idx': range(len(df_new)), 'dfs': df_new})",
   "id": "373c95381e08d49e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gerade = []\n",
    "kinnhaken = []\n",
    "kopfhaken = []\n",
    "\n",
    "#size_of_trainset = int(len(df) * 0.2)\n",
    "#count_per_label = int(size_of_trainset / 3)\n",
    "\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if(df.iloc[i][1].iloc[1].label == \"Gerade\"):\n",
    "        kopfhaken.append(df.iloc[i][1])\n",
    "    elif(df.iloc[i][1].iloc[1].label == \"Kinnhaken\"):\n",
    "        kinnhaken.append(df.iloc[i][1])\n",
    "    elif(df.iloc[i][1].iloc[1].label == \"Kopfhaken\"):\n",
    "        gerade.append(df.iloc[i][1])\n",
    "\n",
    "\n",
    "traindata = gerade+ kinnhaken+ kopfhaken\n",
    "\n",
    "print(len(traindata))"
   ],
   "id": "db83823f47b50d53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x_train,y_train = [],[]\n",
    "\n",
    "for df in traindata:\n",
    "    y_train.append(df[\"label\"].iloc[0])\n",
    "    x_train.append(df.drop(columns=[\"timestamp\", \"label\"]).values)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_train.shape, y_train.shape"
   ],
   "id": "51a7487bab6f5517"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "LABELS = ['Gerade', 'Kinnhaken', 'Kopfhaken']\n",
    "# Set classes to visualize and set the feature names\n",
    "FEATURE_NAMES = ['x', 'y', 'z']\n",
    "CLASSES = ['Gerade', 'Kinnhaken', 'Kopfhaken']\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=len(CLASSES), sharex=True, sharey=False)\n",
    "fig.suptitle('Visualization of different Classes', fontsize=16)\n",
    "fig.set_size_inches(16, 9)\n",
    "x = np.arange(1001)\n",
    "for index, cls in enumerate(CLASSES):\n",
    "    # Set class as label for top subplot of each column\n",
    "    axs[0][index].set(title=cls)\n",
    "    # Get random instance from training data with corresponding class\n",
    "    instance_index = np.random.choice(np.argwhere(y_train == cls).flatten())\n",
    "    instance = x_train[instance_index]\n",
    "    # Plot the feature groups (all axis of one measurement) in the different subplots and add labels\n",
    "    for sublot in [0, 1, 2]:\n",
    "        axs[sublot][index].plot(x, instance[:, sublot])\n",
    "        axs[sublot][index].legend(FEATURE_NAMES[sublot])\n",
    "plt.show()"
   ],
   "id": "e277c228495f3fe3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "_label = ['Gerade', 'Kinnhaken', 'Kopfhaken']\n",
    "category_to_num = {element: num for num, element in enumerate(_label)}\n",
    "numerical_data = np.vectorize(category_to_num.get)(y_train)\n",
    "\n",
    "\n",
    "y_train  = np.array(numerical_data)"
   ],
   "id": "5944d9ce943024c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Definiere das Modell\n",
    "model = Sequential([\n",
    "    InputLayer(shape=x_train[1].shape),\n",
    "    SimpleRNN(units=256, activation='relu', return_sequences=True),\n",
    "    Dropout(rate=0.2),  # Dropout zur Regularisierung\n",
    "    SimpleRNN(units=256, activation='relu', return_sequences=False),\n",
    "    Dropout(rate=0.2),  # Dropout zur Regularisierung\n",
    "    Dense(3, activation='softmax')  # Ausgangsschicht für 3 Klassen\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),  # Angepasste Lernrate\n",
    "              loss=CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Zusammenfassung des Modells ausdrucken\n",
    "print(model.summary())"
   ],
   "id": "dec0fe1c32fbd796"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set parameters for data splitting and training\n",
    "TEST_SIZE = 0.2\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 30\n",
    "\n",
    "\n",
    "# Encode the labels using One-Hot-Encoding\n",
    "y_train_encoded = tf.one_hot(indices=y_train, depth=3)\n",
    "\n",
    "\n",
    "# Train model using validation split\n",
    "history = model.fit(x=x_train, y=y_train_encoded, validation_split=TEST_SIZE, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "# Plot the training histroy\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.set_size_inches(12, 8)\n",
    "fig.suptitle('Training History', fontsize=16)\n",
    "axs[0].plot(history.epoch, history.history['loss'], history.history['val_loss'])\n",
    "axs[0].set(title='Loss', xlabel='Epoch', ylabel='Loss')\n",
    "axs[0].legend(['loss', 'val_loss'])\n",
    "axs[1].plot(history.epoch, history.history['accuracy'], history.history['val_accuracy'])\n",
    "axs[1].set(title='Accuracy', xlabel='Epoch', ylabel='Accuracy')\n",
    "axs[1].legend(['accuracy', 'val_accuracy'])\n",
    "plt.show()"
   ],
   "id": "e2a2e1b88e8d5720"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sk learn modell selectio train_split ",
   "id": "2e14d6577794b97e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6e04d94a0e0b2eba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_predicted = np.argmax(model.predict(x=x_train), axis=1)\n",
    "confusion_matrix = tf.math.confusion_matrix(labels=y_train, predictions=y_predicted)\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(12, 8)\n",
    "sns.heatmap(confusion_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ],
   "id": "f427e9b7ad2f28c8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
